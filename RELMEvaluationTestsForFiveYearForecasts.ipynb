{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RELM Evaluation Tests for Five-Year Forecasts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This demo provides step-by-step instructions on how to invoke RELM **N** (Number) and **L** (Likelihood) evaluation tests for two five-year RELM forecasts. It also explains which data products and images are generated by these evaluation tests, and allows user to view results. This tutorial uses *EvaluationTest.py* CSEP Python module in standalone mode to invoke the tests. Python code, which is a simplified implementation of standalone functionality of the *EvaluationTest.py* module, is also provided in case users want to integrate this CSEP functionality within their custom Python routines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   We use command-line options to provide *EvaluationTest.py* module with necessary information about forecasts and observations that are used for the evaluation.\n",
    " \n",
    "   Comparable forecasts files should be placed in the same directory which represents *forecast group*. Forecast group within CSEP is defined as a collection of comparable forecasts for the same testing region with the same target earthquakes. \n",
    "   This test case uses two RELM five-year forecasts which are stored in *RELMEvaluation/forecasts* directory: \n",
    "   * *helmstetter_et_al.hkj.dat*\n",
    "   * *wiemer_schorlemmer.alm.dat*\n",
    "\n",
    "Observation catalog *catalog.dat* with two events is placed in *RELMEvaluation/observations* directory. Observed events should be stored in ASCII [ZMAP](https://northridge.usc.edu/trac/csep/wiki/catalogZMAPformat) format file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Command-line Options"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following command-line options should be provided to the *EvaluationTest.py* module to invoke evalation tests.\n",
    "\n",
    "* *--forecasts=RELMEvaluation/forecasts* - Directory where *helmstetter_et_al.hkj.dat* and *wiemer_schorlemmer.alm.dat* forecasts files in ASCII [CSEPForecast](https://northridge.usc.edu/trac/csep/wiki/ForecastFormat) format are stored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls RELMEvaluation/forecasts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* *--catalog=RELMEvaluation/observations/catalog.dat* - Path to the observation catalog file in ASCII [ZMAP](https://northridge.usc.edu/trac/csep/wiki/catalogZMAPformat) format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls RELMEvaluation/observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   We use observation catalog which consists of two events and confirms to the ASCII [ZMAP](https://northridge.usc.edu/trac/csep/wiki/catalogZMAPformat) format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat RELMEvaluation/observations/catalog.dat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   Forecast's 5-year testing period is defined by start date of 2006/01/01 inclusively and end date of 2011/01/01 exclusively. Test date for evaluation is set to 2006/9/1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* *--year=2006* - Year of the test date\n",
    "* *--month=9* - Month of the test date\n",
    "* *--day=1* - Day of the test date\n",
    "* *--startDate=2006-01-01* - Start date of the forecast's testing period\n",
    "* *--endDate=2011-01-01* - End date (exclusively) of the forecast's testing period"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   The following options provide information about evaluation tests to invoke:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* *--tests='N L'* - Space-separated list of evaluation tests to invoke\n",
    "* *--testDir=RELMEvaluation/ScriptResults* - Directory to store results to\n",
    "* *--numTestSimulations=1000* - Number of test simulations (default is 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 $CENTERCODE/src/generic/EvaluationTest.py --year=2006 --month=9 --day=1 --startDate=2006-01-01 --endDate=2011-01-01 --catalog=RELMEvaluation/observations/catalog.dat --forecasts=RELMEvaluation/forecasts --tests='N L' --testDir=RELMEvaluation/ScriptResults --numTestSimulations=1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tests Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   This section examines data products that **N** and **L** evaluation tests generated by running above **python3** command.\n",
    "   Please note that each data product, as generated by the CSEP, has corresponding metadata file with identical filename with an additional *.meta* extention. Metadata file captures information on how each data product has been generated and is used for reproducibility of the results only. You can ignore all generated *.meta files for now.\n",
    "   \n",
    "   For example, metadata file for the L-Test summary file has the following content:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat RELMEvaluation/ScriptResults/scec.csep.AllModelsSummary.all.rTest_L-Test.png.*[1-9].meta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Forecast Scale Factor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   Forecast scale factor, that corresponds to the test date of 2006/09/01 within testing period, is captured within *RELMEvaluation/ScriptResults/ForecastScaleFactor.dat* file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat RELMEvaluation/ScriptResults/ForecastScaleFactor.dat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   Summary files (with **scec.csep.AllModelsSummary.** prefix) respresent evaluation tests results for both models.\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls RELMEvaluation/ScriptResults/scec.csep.AllModelsSummary*[1-9]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   XML format of AllModelsSummary files stores results data, and plot of each evaluation test summary is stored in the PNG format image file.\n",
    "   \n",
    "   Summary plot for N-Test is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import Image\n",
    "import glob, shutil\n",
    "\n",
    "# Locate plot file for N-test\n",
    "image_file = glob.glob('RELMEvaluation/ScriptResults/scec.csep.AllModelsSummary.all.rTest_N-Test.png.*[0-9]')[0]\n",
    "print(image_file)\n",
    "\n",
    "# CSEP numerical extensions cause problems with iPython's image display, \n",
    "# create *.png copy of original image file for display\n",
    "image_copy_file = 'RELMEvaluation/ScriptResults/scec.csep.AllModelsSummary.all.rTest_N-Test.png'\n",
    "shutil.copyfile(image_file,\n",
    "               image_copy_file)\n",
    "\n",
    "Image(image_copy_file) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   Summary plot for L-Test is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Locate plot file for L-test\n",
    "image_file = glob.glob('RELMEvaluation/ScriptResults/scec.csep.AllModelsSummary.all.rTest_L-Test.png.*[0-9]')[0]\n",
    "print(image_file)\n",
    "\n",
    "# CSEP numerical extensions cause problems with iPython's image display, \n",
    "# create *.png copy of original image file for display\n",
    "image_copy_file = 'RELMEvaluation/ScriptResults/scec.csep.AllModelsSummary.all.rTest_L-Test.png'\n",
    "shutil.copyfile(image_file,\n",
    "               image_copy_file)\n",
    "\n",
    "Image(image_copy_file) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Individual Model Result Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All results files are placed in the RELMEvaluation/ScriptResults directory:\n",
    "   * XML test results files for *helmstetter_et_al.hkj.dat* forecast are of *scec.csep.RELMTest.rTest_[NL]*-Test_helmstetter_et_al.hkj.xml.* pattern\n",
    "   * XML test results files for *wiemer_schorlemmer.alm.dat* forecast are of *scec.csep.RELMTest.rTest_[NL]*-Test_wiemer_schorlemmer.alm.xml.* pattern"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### N-Test Result Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   N-test results files have *scec.csep.RELMTest.rTest_N-Test* prefix and each XML format file represents N-test evaluation results per model. Plots for N-test evaluation results are stored in corresponding SVG image files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls RELMEvaluation/ScriptResults/scec.csep.RELMTest.rTest_N-Test*[1-9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import SVG\n",
    "\n",
    "# Locate N-test plot file for Helmstetter forecast\n",
    "image_file = glob.glob('RELMEvaluation/ScriptResults/scec.csep.RELMTest.rTest_N-Test_helmstetter_et_al.hkj.svg.*[0-9]')[0]\n",
    "print(image_file)\n",
    "SVG(image_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Locate N-test plot file for WS forecast\n",
    "image_file = glob.glob('RELMEvaluation/ScriptResults/scec.csep.RELMTest.rTest_N-Test_wiemer_schorlemmer.alm.svg.*[0-9]')[0]\n",
    "print(image_file)\n",
    "SVG(image_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### L-Test Result Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   L-test result files have *scec.csep.RELMTest.rTest_L-Test* prefix and each XML format file represents L-test evaluation results per model. Plots for L-test evaluation results are stored in corresponding SVG image files.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls RELMEvaluation/ScriptResults/scec.csep.RELMTest.rTest_L-Test*[1-9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Locate L-test plot file for Helmstetter forecast\n",
    "image_file = glob.glob('RELMEvaluation/ScriptResults/scec.csep.RELMTest.rTest_L-Test_helmstetter_et_al.hkj.svg.*[0-9]')[0]\n",
    "print('L-test result plot for helmstetter_et_al.hkj forecast:', image_file)\n",
    "SVG(filename=image_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Locate L-test plot file for WS forecast\n",
    "image_file = glob.glob('RELMEvaluation/ScriptResults/scec.csep.RELMTest.rTest_L-Test_wiemer_schorlemmer.alm.svg.*[0-9]')[0]\n",
    "print('L-test result plot for wiemer_schorlemmer.alm forecast:', image_file)\n",
    "SVG(image_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding Your Forecast to the Test Case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   To add your own forecast to the test case, just place your forecast file in ASCII [CSEPForecast](https://northridge.usc.edu/trac/csep/wiki/ForecastFormat) format under *RELMEvaluation/forecasts* directory, and re-run the test case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python Code to Run Evaluation Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   Detailed Python code below provides (simplified) behind the scenes details of what provided above **python3** command does when *EvaluationTest.py* module is invoked in standalone mode.\n",
    "\n",
    "   Please note that we use different *RELMEvaluation/PythonResults* directory to store results data to when invoking the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "# Import CSEP modules\n",
    "import CSEPUtils\n",
    "from ForecastGroup import ForecastGroup\n",
    "from EvaluationTest import EvaluationTest\n",
    "from PostProcess import PostProcess\n",
    "\n",
    "# Path to the observation catalog\n",
    "catalog_file = 'RELMEvaluation/observations/catalog.dat'\n",
    "# Path to the forecast group directory\n",
    "forecast_dir = 'RELMEvaluation/forecasts'\n",
    "# Path to the evaluation test results (please note it's different from above 'RELMEvaluation/results')\n",
    "results_dir = 'RELMEvaluation/PythonResults'\n",
    "test_list = 'N L'\n",
    "\n",
    "# Start date for the testing period\n",
    "start_date = datetime.datetime(2006, 1, 1)\n",
    "\n",
    "# End date for the testing period\n",
    "end_date = datetime.datetime(2011, 1, 1)\n",
    "\n",
    "# Test date for evaluation\n",
    "test_date = datetime.datetime(2006, 9, 1)\n",
    "\n",
    "forecast_duration = CSEPUtils.decimalYear(end_date) - \\\n",
    "                             CSEPUtils.decimalYear(start_date)\n",
    "\n",
    "# Create PostProcess object (catalog filtering thresholds) \n",
    "# and pass it to the ForecastGroup to evaluate\n",
    "min_magnitude = 4.95\n",
    "max_depth = 30.0\n",
    "post_process = PostProcess(min_magnitude,\n",
    "                           max_depth,\n",
    "                           forecast_duration,\n",
    "                           catalog_file)\n",
    "\n",
    "post_process.startDate(start_date)\n",
    "post_process.endDate(end_date)\n",
    "\n",
    "# Instantiate forecast group for the tests\n",
    "forecast_group = ForecastGroup(forecast_dir,\n",
    "                               post_process,\n",
    "                               test_list)\n",
    "\n",
    "# Run evaluation tests        \n",
    "for each_set in forecast_group.tests:\n",
    "    for each_test in each_set:\n",
    "        # Use the same directory for catalog data and test results: options.test_dir\n",
    "        print('Running %s evaluation test' %each_test.Type)\n",
    "        each_test.run(test_date,\n",
    "                      '.',\n",
    "                      results_dir)\n",
    "         \n",
    "        # Update cumulative summaries if any\n",
    "        each_test.resultData()\n",
    "print('Done with %s evaluation tests for %s group.' %(test_list, forecast_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
